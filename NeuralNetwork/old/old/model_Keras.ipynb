{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the plaidml backend\n",
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Input, Activation, Dropout, GlobalAveragePooling2D, \\\n",
    "    BatchNormalization, concatenate, AveragePooling2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Permute\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import GRU, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "from py2neo import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"/Users/david/TFM_DATA/spec\"\n",
    "data_files = os.listdir(PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH triplets\n",
    "PATH_TRIPLETS = os.path.join(\"..\",\"triplets\", \"triplets.csv\")\n",
    "df = pd.read_csv(PATH_TRIPLETS, delimiter= \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List\n",
    "triplets_input = list(df[\"output\"])\n",
    "size_triplets_input = len(triplets_input)\n",
    "\n",
    "# Sample\n",
    "triplets_input_v1 = triplets_input[:1000]\n",
    "size_triplets_input = len(triplets_input_v1)\n",
    "\n",
    "# Artists\n",
    "artists_labels = np.sort(df.a1.unique())\n",
    "df_artists = pd.DataFrame(data = artists_labels, columns = [\"artist\"]).reset_index()\n",
    "df_artists.columns = [\"id\",\"artist\"]\n",
    "\n",
    "# DF Artists with artists as index\n",
    "df_artists_index = df_artists.set_index(\"artist\")\n",
    "num_artists = df_artists.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalan music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(bolt = True, host = \"localhost\", name = \"Spotify\", user = \"neo4j\", password = \"qrks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        MATCH (g:Genre)-[:GEN_ART]->(a:Artist)\n",
    "        MATCH (a:Artist)-[:REL_ART]->(a2:Artist)\n",
    "        WHERE toLower(g.genre_id) =~ '.*catala.*'\n",
    "        RETURN a.artist_id, a2.artist_id\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = graph.run(query)\n",
    "dfneo = pd.DataFrame.from_records(cursor, columns=cursor.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneo.to_csv(\"dfneo_CAT.csv\", sep = \";\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_art_cat = set()\n",
    "for ii, row in dfneo.iterrows():\n",
    "    set_art_cat.add(row[\"a.artist_id\"])\n",
    "    set_art_cat.add(row[\"a2.artist_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: select couple of artists\n",
    "sel_art = list(set_art_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists_cat = df_artists[df_artists.artist.isin(set_art_cat)][\"artist\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists_cat[\"id\"] = df_artists_cat.index\n",
    "df_artists_cat.drop('index', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists_gen = df_artists_cat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_mat = keras.utils.to_categorical(df_artists_index.id)\n",
    "labels_mat = keras.utils.to_categorical(df_artists_gen.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dict()\n",
    "for i, row in df_artists_gen.iterrows():\n",
    "    labels[row.artist] = labels_mat[row.id,].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_artists = len(labels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = df[df.a1.isin(set_art_cat)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataframe Classiset_art_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_gen[[\"a1\",\"tr1\",\"win1\",\"ini1\",\"fin1\"]].copy()\n",
    "df2 = df_gen[[\"a1\",\"tr2\",\"win2\",\"ini2\",\"fin2\"]].copy()\n",
    "\n",
    "#Â Column names\n",
    "colnames = [\"art\",\"tr\",\"win\",\"ini\",\"fin\"]\n",
    "df1.columns = colnames\n",
    "df2.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2])\n",
    "df_concat.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat[\"tr\"] = df_concat.tr + \"__\" + df_concat.win.astype(str) +  \\\n",
    "    \"__\" + df_concat.ini.astype(str) + \"__\" + df_concat.fin.astype(str) + \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1915, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.reset_index().drop(\"index\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, df, scaling, x_col, y_col=None, batch_size=10, num_classes=None, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df\n",
    "        self.indices = self.df.index.tolist()\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.dim = (int(x_col / scaling), int(y_col / scaling), 1) #one input channel\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.indices[k] for k in idx]\n",
    "        \n",
    "        X, y = self.__get_data(batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n",
    "        \n",
    "        # Get the list of image files and corresponding artists\n",
    "        df_imgs_files = self.df.iloc[batch]\n",
    "        \n",
    "        list_imgs = list(df_imgs_files[\"tr\"])\n",
    "        list_art = list(df_imgs_files[\"art\"])\n",
    "        \n",
    "        for ii in range(len(list_imgs)):\n",
    "            \n",
    "            # Read image using cv2\n",
    "            path_img = os.path.join(PATH_DATA, list_imgs[ii])\n",
    "            img = cv2.cvtColor(cv2.imread(path_img), cv2.COLOR_BGR2GRAY)\n",
    "            img = np.round(img / 255.,5)\n",
    "            img = cv2.resize(img, (self.y_col, self.x_col))\n",
    "            img = cv2.resize(img, (self.dim[1], self.dim[0]))\n",
    "            img = np.expand_dims(img, axis = 2) # add the dimension of the channel \n",
    "            \n",
    "            # Put it into X matrix\n",
    "            X[ii,] = img\n",
    "            y[ii] = labels[list_art[ii]]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col, y_col = (256, 937)\n",
    "batch_size = 5\n",
    "num_classes = df_artists.shape[0]\n",
    "shuffle = False\n",
    "scaling = 2\n",
    "\n",
    "training_generator = DataGenerator(df=df_concat, \n",
    "                                   scaling = scaling,\n",
    "                                   x_col=x_col, \n",
    "                                   y_col=y_col,\n",
    "                                   batch_size=batch_size, \n",
    "                                   num_classes=num_artists,\n",
    "                                   shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(conv_x, filters):\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "    conv_x = Conv2D(filters, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(conv_x)\n",
    "    conv_x = Dropout(0.2)(conv_x)\n",
    "\n",
    "    return conv_x\n",
    "\n",
    "\n",
    "def dense_block(block_x, filters, growth_rate, layers_in_block):\n",
    "    for i in range(layers_in_block):\n",
    "        each_layer = conv_layer(block_x, growth_rate)\n",
    "        block_x = concatenate([block_x, each_layer], axis=-1)\n",
    "        filters += growth_rate\n",
    "\n",
    "    return block_x, filters\n",
    "\n",
    "def transition_block(trans_x, tran_filters):\n",
    "    trans_x = BatchNormalization()(trans_x)\n",
    "    trans_x = Activation('relu')(trans_x)\n",
    "    trans_x = Conv2D(tran_filters, (1, 1), kernel_initializer='he_uniform', padding='same', use_bias=False)(trans_x)\n",
    "    trans_x = AveragePooling2D((2, 2), strides=(2, 2))(trans_x)\n",
    "\n",
    "    return trans_x, tran_filters\n",
    "\n",
    "def dense_net(filters, growth_rate, classes, dense_block_size, layers_in_block, in_shape):\n",
    "    input_img = Input(shape=in_shape)\n",
    "    x = Conv2D(3, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(input_img)\n",
    "\n",
    "    dense_x = BatchNormalization()(x)\n",
    "    dense_x = Activation('relu')(x)\n",
    "\n",
    "    dense_x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(dense_x)\n",
    "    for block in range(dense_block_size - 1):\n",
    "        dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n",
    "        dense_x, filters = transition_block(dense_x, filters)\n",
    "\n",
    "    dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n",
    "    dense_x = BatchNormalization()(dense_x)\n",
    "    dense_x = Activation('relu')(dense_x)\n",
    "    dense_x = GlobalAveragePooling2D()(dense_x)\n",
    "\n",
    "    output = Dense(classes, activation='softmax')(dense_x)\n",
    "\n",
    "    return Model(input_img, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_CNN(input_shape, num_classes):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(2, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(4, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_CNN2(input_shape, num_classes):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(16, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(4, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRNN2D(X_shape, nb_classes):\n",
    "    '''\n",
    "    Model used for evaluation in paper. Inspired by K. Choi model in:\n",
    "    https://github.com/keunwoochoi/music-auto_tagging-keras/blob/master/music_tagger_crnn.py\n",
    "    '''\n",
    "\n",
    "    nb_layers = 4  # number of convolutional layers\n",
    "    nb_filters = [64, 128, 128, 128]  # filter sizes\n",
    "    kernel_size = (3, 3)  # convolution kernel size\n",
    "    activation = 'elu'  # activation function to use after each layer\n",
    "    pool_size = [(2, 2), (4, 2), (4, 2), (4, 2),\n",
    "                 (4, 2)]  # size of pooling area\n",
    "\n",
    "    # shape of input data (frequency, time, channels)\n",
    "    input_shape = X_shape\n",
    "    frequency_axis = 1\n",
    "    time_axis = 2\n",
    "    channel_axis = 3\n",
    "\n",
    "    # Create sequential model and normalize along frequency axis\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(axis=frequency_axis, input_shape=input_shape))\n",
    "\n",
    "    # First convolution layer specifies shape\n",
    "    model.add(Conv2D(nb_filters[0], kernel_size=kernel_size, padding='same',\n",
    "                     data_format=\"channels_last\",\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size[0], strides=pool_size[0]))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    # Add more convolutional layers\n",
    "    for layer in range(nb_layers - 1):\n",
    "        # Convolutional layer\n",
    "        model.add(Conv2D(nb_filters[layer + 1], kernel_size=kernel_size,\n",
    "                         padding='same'))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(BatchNormalization(\n",
    "            axis=channel_axis))  # Improves overfitting/underfitting\n",
    "        model.add(MaxPooling2D(pool_size=pool_size[layer + 1],\n",
    "                               strides=pool_size[layer + 1]))  # Max pooling\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        # Reshaping input for recurrent layer\n",
    "    # (frequency, time, channels) --> (time, frequency, channel)\n",
    "    model.add(Permute((time_axis, frequency_axis, channel_axis)))\n",
    "    resize_shape = model.output_shape[2] * model.output_shape[3]\n",
    "    model.add(Reshape((model.output_shape[1], resize_shape)))\n",
    "\n",
    "    # recurrent layer\n",
    "    model.add(GRU(32, return_sequences=True))\n",
    "    model.add(GRU(32, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_580x.0\"\n"
     ]
    }
   ],
   "source": [
    "#dense_block_size = 4\n",
    "#layers_in_block = 2\n",
    "\n",
    "#growth_rate = 2\n",
    "in_shape = (int(x_col / scaling), int(y_col / scaling), 1)\n",
    "classes = num_artists\n",
    "\n",
    "#model = dense_net(growth_rate * 2, growth_rate, classes, dense_block_size, layers_in_block, in_shape)\n",
    "#model = quick_CNN2(input_shape=in_shape, num_classes = classes)\n",
    "model = CRNN2D(in_shape, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics_folder='metrics'\n",
    "save_weights_folder='weights'\n",
    "load_checkpoint = False\n",
    "early_stop=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = os.path.join(save_weights_folder, 'crnn_v1_epoch_{epoch:02d}.hdf5')\n",
    "os.makedirs(save_weights_folder, exist_ok=True)\n",
    "os.makedirs(save_metrics_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from scratch (no checkpoint)\n"
     ]
    }
   ],
   "source": [
    "if load_checkpoint:\n",
    "    print(\"Looking for previous weights...\")\n",
    "    if isfile(weights):\n",
    "        print('Checkpoint file detected. Loading weights.')\n",
    "        model.load_weights(weights)\n",
    "    else:\n",
    "        print('No checkpoint file detected.  Starting from scratch.')\n",
    "else:\n",
    "    print('Starting from scratch (no checkpoint)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=weights,\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False,\n",
    "                                   save_weights_only=True,\n",
    "                                   period=1,\n",
    "                                   monitor = 'categorical_crossentropy')\n",
    "earlystopper = EarlyStopping(monitor='categorical_crossentropy', min_delta=0,\n",
    "                             patience=early_stop, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "optimizer = Adam(lr=lr)\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 4686 of 5476 operations complete\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator = training_generator, \n",
    "          epochs=epochs, \n",
    "          callbacks=[checkpointer, earlystopper],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
